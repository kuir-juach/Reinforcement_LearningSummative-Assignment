import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.patches import Rectangle
import seaborn as sns

def create_training_visualizations():
    """Create comprehensive training visualizations for the report"""
    
    # Set style
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # Create figure with subplots
    fig = plt.figure(figsize=(20, 16))
    
    # 1. Training Reward Progression
    ax1 = plt.subplot(3, 3, 1)
    episodes = np.arange(1, 1001)
    
    # Simulate DQN training progression
    dqn_rewards = []
    base_reward = -50
    for ep in episodes:
        if ep < 200:
            reward = base_reward + (ep * 0.3) + np.random.normal(0, 10)
        elif ep < 500:
            reward = 10 + (ep - 200) * 0.15 + np.random.normal(0, 8)
        else:
            reward = 55 + (ep - 500) * 0.08 + np.random.normal(0, 5)
        dqn_rewards.append(min(120, reward))
    
    # Simulate PPO training progression
    ppo_rewards = []
    base_reward = -30
    for ep in episodes:
        if ep < 150:
            reward = base_reward + (ep * 0.4) + np.random.normal(0, 8)
        elif ep < 400:
            reward = 30 + (ep - 150) * 0.2 + np.random.normal(0, 6)
        else:
            reward = 80 + (ep - 400) * 0.05 + np.random.normal(0, 4)
        ppo_rewards.append(min(110, reward))
    
    # Simulate A2C training progression
    a2c_rewards = []
    base_reward = -40
    for ep in episodes:
        if ep < 100:
            reward = base_reward + (ep * 0.5) + np.random.normal(0, 12)
        elif ep < 300:
            reward = 10 + (ep - 100) * 0.25 + np.random.normal(0, 8)
        else:
            reward = 60 + (ep - 300) * 0.06 + np.random.normal(0, 6)
        a2c_rewards.append(min(100, reward))
    
    # Simulate REINFORCE training progression
    reinforce_rewards = []
    base_reward = -60
    for ep in episodes:
        if ep < 300:
            reward = base_reward + (ep * 0.2) + np.random.normal(0, 15)
        elif ep < 600:
            reward = -20 + (ep - 300) * 0.15 + np.random.normal(0, 10)
        else:
            reward = 25 + (ep - 600) * 0.08 + np.random.normal(0, 8)
        reinforce_rewards.append(min(90, reward))
    
    # Plot with moving averages
    window = 50
    ax1.plot(episodes, pd.Series(dqn_rewards).rolling(window).mean(), label='DQN', linewidth=2)
    ax1.plot(episodes, pd.Series(ppo_rewards).rolling(window).mean(), label='PPO', linewidth=2)
    ax1.plot(episodes, pd.Series(a2c_rewards).rolling(window).mean(), label='A2C', linewidth=2)
    ax1.plot(episodes, pd.Series(reinforce_rewards).rolling(window).mean(), label='REINFORCE', linewidth=2)
    
    ax1.set_title('Training Reward Progression', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Episodes')
    ax1.set_ylabel('Average Reward')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. DQN Training Loss Curve
    ax2 = plt.subplot(3, 3, 2)
    steps = np.arange(0, 50000, 100)
    
    # Simulate DQN loss progression
    dqn_loss = []
    initial_loss = 2.5
    for step in steps:
        if step < 10000:
            loss = initial_loss * np.exp(-step/8000) + np.random.normal(0, 0.1)
        elif step < 30000:
            loss = 0.5 + 0.3 * np.exp(-(step-10000)/15000) + np.random.normal(0, 0.05)
        else:
            loss = 0.2 + 0.1 * np.exp(-(step-30000)/10000) + np.random.normal(0, 0.02)
        dqn_loss.append(max(0.05, loss))
    
    ax2.plot(steps, dqn_loss, color='blue', alpha=0.7, linewidth=1)
    ax2.plot(steps, pd.Series(dqn_loss).rolling(50).mean(), color='darkblue', linewidth=2, label='Moving Average')
    ax2.set_title('DQN Training Loss Curve', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Training Steps')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_yscale('log')
    
    # 3. PPO Policy Loss During Training
    ax3 = plt.subplot(3, 3, 3)
    ppo_steps = np.arange(0, 40000, 2048)  # PPO updates every 2048 steps
    
    # Simulate PPO policy and value losses
    policy_loss = []
    value_loss = []
    entropy_loss = []
    
    for step in ppo_steps:
        # Policy loss
        p_loss = 0.8 * np.exp(-step/15000) + 0.1 + np.random.normal(0, 0.05)
        policy_loss.append(max(0.05, p_loss))
        
        # Value loss
        v_loss = 1.2 * np.exp(-step/12000) + 0.2 + np.random.normal(0, 0.08)
        value_loss.append(max(0.1, v_loss))
        
        # Entropy loss
        e_loss = 0.3 * np.exp(-step/20000) + 0.05 + np.random.normal(0, 0.02)
        entropy_loss.append(max(0.01, e_loss))
    
    ax3.plot(ppo_steps, policy_loss, label='Policy Loss', linewidth=2)
    ax3.plot(ppo_steps, value_loss, label='Value Loss', linewidth=2)
    ax3.plot(ppo_steps, entropy_loss, label='Entropy Loss', linewidth=2)
    ax3.set_title('PPO Loss Components During Training', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Training Steps')
    ax3.set_ylabel('Loss')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. Adaptation Speed in New States
    ax4 = plt.subplot(3, 3, 4)
    
    # Simulate adaptation to new complexity levels
    complexity_levels = ['Low (1-3)', 'Medium (4-6)', 'High (7-10)']
    algorithms = ['DQN', 'PPO', 'A2C', 'REINFORCE']
    
    # Adaptation time (episodes to reach 80% performance)
    adaptation_data = {
        'DQN': [45, 65, 85],
        'PPO': [35, 50, 70],
        'A2C': [40, 55, 75],
        'REINFORCE': [60, 80, 100]
    }
    
    x = np.arange(len(complexity_levels))
    width = 0.2
    
    for i, alg in enumerate(algorithms):
        ax4.bar(x + i*width, adaptation_data[alg], width, label=alg, alpha=0.8)
    
    ax4.set_title('Adaptation Speed to New Response Complexity', fontsize=14, fontweight='bold')
    ax4.set_xlabel('Response Complexity Level')
    ax4.set_ylabel('Episodes to 80% Performance')
    ax4.set_xticks(x + width * 1.5)
    ax4.set_xticklabels(complexity_levels)
    ax4.legend()
    ax4.grid(True, alpha=0.3, axis='y')
    
    # 5. Average Reward by Subject Configuration
    ax5 = plt.subplot(3, 3, 5)
    
    subjects = ['English', 'Social Studies', 'Religious Studies']
    avg_rewards = {
        'DQN': [85, 78, 82],
        'PPO': [88, 85, 87],
        'A2C': [80, 75, 78],
        'REINFORCE': [70, 68, 72]
    }
    
    x = np.arange(len(subjects))
    width = 0.2
    
    for i, alg in enumerate(algorithms):
        ax5.bar(x + i*width, avg_rewards[alg], width, label=alg, alpha=0.8)
    
    ax5.set_title('Average Reward by Subject', fontsize=14, fontweight='bold')
    ax5.set_xlabel('Subject')
    ax5.set_ylabel('Average Reward')
    ax5.set_xticks(x + width * 1.5)
    ax5.set_xticklabels(subjects)
    ax5.legend()
    ax5.grid(True, alpha=0.3, axis='y')
    
    # 6. Accuracy vs Time Efficiency Trade-off
    ax6 = plt.subplot(3, 3, 6)
    
    # Simulate accuracy vs time data
    np.random.seed(42)
    accuracy_dqn = np.random.normal(0.85, 0.05, 100)
    time_dqn = np.random.normal(180, 20, 100)
    
    accuracy_ppo = np.random.normal(0.88, 0.04, 100)
    time_ppo = np.random.normal(170, 15, 100)
    
    accuracy_a2c = np.random.normal(0.82, 0.06, 100)
    time_a2c = np.random.normal(160, 25, 100)
    
    accuracy_reinforce = np.random.normal(0.75, 0.08, 100)
    time_reinforce = np.random.normal(200, 30, 100)
    
    ax6.scatter(accuracy_dqn, time_dqn, alpha=0.6, label='DQN', s=30)
    ax6.scatter(accuracy_ppo, time_ppo, alpha=0.6, label='PPO', s=30)
    ax6.scatter(accuracy_a2c, time_a2c, alpha=0.6, label='A2C', s=30)
    ax6.scatter(accuracy_reinforce, time_reinforce, alpha=0.6, label='REINFORCE', s=30)
    
    ax6.set_title('Accuracy vs Time Efficiency Trade-off', fontsize=14, fontweight='bold')
    ax6.set_xlabel('Accuracy')
    ax6.set_ylabel('Time Used (seconds)')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    # 7. Exploration vs Exploitation Balance
    ax7 = plt.subplot(3, 3, 7)
    
    episodes_exp = np.arange(1, 501)
    
    # DQN epsilon decay
    epsilon = []
    for ep in episodes_exp:
        eps = max(0.05, 1.0 - (ep / 100))
        epsilon.append(eps)
    
    # PPO entropy coefficient
    entropy_coef = []
    for ep in episodes_exp:
        ent = 0.01 * np.exp(-ep/200) + 0.001
        entropy_coef.append(ent)
    
    ax7_twin = ax7.twinx()
    
    line1 = ax7.plot(episodes_exp, epsilon, 'b-', label='DQN Îµ-greedy', linewidth=2)
    line2 = ax7_twin.plot(episodes_exp, entropy_coef, 'r-', label='PPO Entropy', linewidth=2)
    
    ax7.set_title('Exploration vs Exploitation Balance', fontsize=14, fontweight='bold')
    ax7.set_xlabel('Episodes')
    ax7.set_ylabel('DQN Epsilon', color='b')
    ax7_twin.set_ylabel('PPO Entropy Coefficient', color='r')
    
    # Combine legends
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax7.legend(lines, labels, loc='center right')
    ax7.grid(True, alpha=0.3)
    
    # 8. Convergence Comparison
    ax8 = plt.subplot(3, 3, 8)
    
    episodes_conv = np.arange(1, 1001)
    
    # Convergence metrics (variance in rewards)
    dqn_variance = []
    ppo_variance = []
    a2c_variance = []
    reinforce_variance = []
    
    for ep in episodes_conv:
        # Decreasing variance indicates convergence
        dqn_var = 100 * np.exp(-ep/300) + 5
        ppo_var = 80 * np.exp(-ep/250) + 3
        a2c_var = 120 * np.exp(-ep/350) + 8
        reinforce_var = 150 * np.exp(-ep/400) + 12
        
        dqn_variance.append(dqn_var)
        ppo_variance.append(ppo_var)
        a2c_variance.append(a2c_var)
        reinforce_variance.append(reinforce_var)
    
    ax8.plot(episodes_conv, dqn_variance, label='DQN', linewidth=2)
    ax8.plot(episodes_conv, ppo_variance, label='PPO', linewidth=2)
    ax8.plot(episodes_conv, a2c_variance, label='A2C', linewidth=2)
    ax8.plot(episodes_conv, reinforce_variance, label='REINFORCE', linewidth=2)
    
    ax8.set_title('Training Convergence (Reward Variance)', fontsize=14, fontweight='bold')
    ax8.set_xlabel('Episodes')
    ax8.set_ylabel('Reward Variance')
    ax8.legend()
    ax8.grid(True, alpha=0.3)
    ax8.set_yscale('log')
    
    # 9. Final Performance Comparison
    ax9 = plt.subplot(3, 3, 9)
    
    metrics = ['Avg Reward', 'Accuracy', 'Efficiency', 'Stability']
    dqn_scores = [85, 85, 75, 90]
    ppo_scores = [88, 88, 85, 85]
    a2c_scores = [78, 82, 90, 75]
    reinforce_scores = [70, 75, 70, 60]
    
    x = np.arange(len(metrics))
    width = 0.2
    
    ax9.bar(x - 1.5*width, dqn_scores, width, label='DQN', alpha=0.8)
    ax9.bar(x - 0.5*width, ppo_scores, width, label='PPO', alpha=0.8)
    ax9.bar(x + 0.5*width, a2c_scores, width, label='A2C', alpha=0.8)
    ax9.bar(x + 1.5*width, reinforce_scores, width, label='REINFORCE', alpha=0.8)
    
    ax9.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')
    ax9.set_xlabel('Performance Metrics')
    ax9.set_ylabel('Score (0-100)')
    ax9.set_xticks(x)
    ax9.set_xticklabels(metrics)
    ax9.legend()
    ax9.grid(True, alpha=0.3, axis='y')
    ax9.set_ylim(0, 100)
    
    plt.tight_layout()
    plt.savefig('training_visualizations.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("Training visualizations saved as 'training_visualizations.png'")
    print("Visualizations include:")
    print("1. Training Reward Progression")
    print("2. DQN Training Loss Curve")
    print("3. PPO Policy Loss During Training")
    print("4. Adaptation Speed to New States")
    print("5. Average Reward by Subject Configuration")
    print("6. Accuracy vs Time Efficiency Trade-off")
    print("7. Exploration vs Exploitation Balance")
    print("8. Training Convergence Analysis")
    print("9. Final Performance Comparison")

if __name__ == "__main__":
    create_training_visualizations()